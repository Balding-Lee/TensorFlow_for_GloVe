{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cac720ec-6701-43df-82b1-2b5bda883493",
   "metadata": {},
   "source": [
    "# CS224n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad4b8e-7586-4bf5-a242-20d952815918",
   "metadata": {},
   "source": [
    "## SVD练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60edca1-1fb9-48f5-b957-925b7edfe165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = ['I', 'like', 'enjoy', 'deep', 'learning', 'NLP', 'flying', '.']\n",
    "X = np.array([[0, 2, 1, 0, 0, 0, 0, 0],\n",
    "              [2, 0, 0, 1, 0, 1, 0, 0],\n",
    "              [1, 0, 0, 0, 0, 0, 1, 0],\n",
    "              [0, 1, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0, 1],\n",
    "              [0, 1, 0, 0, 0, 0, 0, 1],\n",
    "              [0, 0, 1, 0, 0, 0, 0, 1],\n",
    "              [0, 0, 0, 0, 1, 1, 1, 0]])\n",
    "\n",
    "U, s, Vh = np.linalg.svd(X, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67f5f93-01da-48d6-a7bc-11013f29500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U [[-5.24124930e-01 -5.72859145e-01  9.54463014e-02  3.83228493e-01\n",
      "  -1.76963375e-01 -1.76092183e-01 -4.19185600e-01 -5.57702732e-02]\n",
      " [-5.94438071e-01  6.30120664e-01 -1.70207779e-01  3.10038363e-01\n",
      "   1.84062339e-01 -2.34777849e-01  1.29535474e-01  1.36813128e-01]\n",
      " [-2.56274005e-01  2.74017533e-01  1.59810848e-01  3.88578059e-16\n",
      "  -5.78984617e-01  6.36550929e-01 -9.15933995e-16 -3.05414877e-01]\n",
      " [-2.85637408e-01 -2.47912130e-01  3.54610324e-01 -7.31901294e-02\n",
      "   4.45784489e-01  8.36141432e-02  5.48721075e-01 -4.68012411e-01]\n",
      " [-1.93139313e-01  3.38495048e-02 -5.00790405e-01 -4.28462480e-01\n",
      "   3.47110226e-01  1.55483227e-01 -4.68663749e-01 -4.03576557e-01]\n",
      " [-3.05134684e-01 -2.93988990e-01 -2.23433593e-01 -1.91614246e-01\n",
      "   1.27460940e-01  4.91219408e-01  2.09592800e-01  6.57535375e-01]\n",
      " [-1.82489837e-01 -1.61027767e-01 -3.97842428e-01 -3.83228493e-01\n",
      "  -5.12923221e-01 -4.27574417e-01  4.19185600e-01 -1.18313828e-01]\n",
      " [-2.46898426e-01  1.57254762e-01  5.92991677e-01 -6.20076727e-01\n",
      "  -3.21868120e-02 -2.31065080e-01 -2.59070949e-01  2.37976916e-01]]\n",
      "s [2.75726275 2.678248   1.89221277 1.61803399 1.19154564 0.94833983\n",
      " 0.61803399 0.56999221]\n",
      "Vh [[-5.24124930e-01 -5.94438071e-01 -2.56274005e-01 -2.85637408e-01\n",
      "  -1.93139313e-01 -3.05134684e-01 -1.82489837e-01 -2.46898426e-01]\n",
      " [ 5.72859145e-01 -6.30120664e-01 -2.74017533e-01  2.47912130e-01\n",
      "  -3.38495048e-02  2.93988990e-01  1.61027767e-01 -1.57254762e-01]\n",
      " [-9.54463014e-02  1.70207779e-01 -1.59810848e-01 -3.54610324e-01\n",
      "   5.00790405e-01  2.23433593e-01  3.97842428e-01 -5.92991677e-01]\n",
      " [ 3.83228493e-01  3.10038363e-01 -4.44089210e-16 -7.31901294e-02\n",
      "  -4.28462480e-01 -1.91614246e-01 -3.83228493e-01 -6.20076727e-01]\n",
      " [-1.76963375e-01  1.84062339e-01 -5.78984617e-01  4.45784489e-01\n",
      "   3.47110226e-01  1.27460940e-01 -5.12923221e-01 -3.21868120e-02]\n",
      " [ 1.76092183e-01  2.34777849e-01 -6.36550929e-01 -8.36141432e-02\n",
      "  -1.55483227e-01 -4.91219408e-01  4.27574417e-01  2.31065080e-01]\n",
      " [ 4.19185600e-01 -1.29535474e-01 -9.99200722e-16 -5.48721075e-01\n",
      "   4.68663749e-01 -2.09592800e-01 -4.19185600e-01  2.59070949e-01]\n",
      " [-5.57702732e-02  1.36813128e-01 -3.05414877e-01 -4.68012411e-01\n",
      "  -4.03576557e-01  6.57535375e-01 -1.18313828e-01  2.37976916e-01]]\n"
     ]
    }
   ],
   "source": [
    "print('U', U)\n",
    "print('s', s)\n",
    "print('Vh', Vh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba07a0f-08ea-49de-8525-e35f498f23fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(len(words)):\n",
    "    plt.scatter(U[i, 0], U[i, 1])\n",
    "    plt.text(U[i, 0], U[i, 1], words[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf6502-9881-4d37-a404-1a624f2007c8",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26936fc-fb7c-4681-931f-d15bd47cd3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\anaconda\\envs\\tf18\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\anaconda\\envs\\tf18\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\anaconda\\envs\\tf18\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\anaconda\\envs\\tf18\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\anaconda\\envs\\tf18\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\anaconda\\envs\\tf18\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import bz2\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import lil_matrix\n",
    "import nltk # standard preprocessing\n",
    "import operator # sorting items in dictionary by value\n",
    "#nltk.download() #tokenizers/punkt/PY3/english.pickle\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29b68ce-98bb-4f7b-a655-86f5c4d748b2",
   "metadata": {},
   "source": [
    "# 1 数据集下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb3eb50-0a2b-421c-ac51-eb1ee99f1653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified wikipedia2text-extracted.txt.bz2\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.evanjones.ca/software/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('wikipedia2text-extracted.txt.bz2', 18377035)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83090d11-96f3-471c-bfba-d96e6a4e5cd7",
   "metadata": {},
   "source": [
    "# 2 读取数据集\n",
    "\n",
    "该步骤主要包含：将数据读取出来成为string，将数据全部转换为小写，对数据进行分词操作。每次读取读取1M数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6ad8b9-6936-4f44-bba2-5b8159e414b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Data size 3361192\n",
      "Example words (start):  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
      "Example words (end):  ['favorable', 'long-term', 'outcomes', 'for', 'around', 'half', 'of', 'those', 'diagnosed', 'with']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Extract the first file enclosed in a zip file as a list of words\n",
    "    and pre-processes it using the nltk python library\n",
    "    \"\"\"\n",
    "\n",
    "    with bz2.BZ2File(filename) as f:\n",
    "\n",
    "        data = []\n",
    "        file_size = os.stat(filename).st_size\n",
    "        chunk_size = 1024 * 1024 # reading 1 MB at a time as the dataset is moderately large\n",
    "        print('Reading data...')\n",
    "        for i in range(ceil(file_size//chunk_size)+1):\n",
    "            bytes_to_read = min(chunk_size,file_size-(i*chunk_size))\n",
    "            file_string = f.read(bytes_to_read).decode('utf-8')\n",
    "            file_string = file_string.lower()  # 将数据转换为小写\n",
    "            # tokenizes a string to words residing in a list\n",
    "            file_string = nltk.word_tokenize(file_string)  # 分词\n",
    "            data.extend(file_string)\n",
    "    return data\n",
    "\n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))\n",
    "token_count = len(words)\n",
    "\n",
    "print('Example words (start): ',words[:10])\n",
    "print('Example words (end): ',words[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd0cdd2-b774-4594-b81c-531eafd913d7",
   "metadata": {},
   "source": [
    "# 3 创建词典\n",
    "\n",
    "根据以下的规则进行词典的创建. 为了方便理解以下的元素，采用 \"I like to go to school\"作为例子.\n",
    "\n",
    "* `dictionary`: 词语与ID之间的映射关系 (e.g. {'I': 0, 'like': 1, 'to': 2, 'go': 3, 'school': 4})\n",
    "* `reverse_dictionary`: ID与词语之间的映射关系 (e.g. {0: 'I', 1: 'like', 2: 'to', 3: 'go', 4: 'school'})\n",
    "* `count`: 列表，列表中每个元素是个元组，每个元组中的元素为单词以及频率 (word, frequency) (e.g. [('I', 1), ('like', 1), ('to', 2), ('go', 1), ('school', 1)])\n",
    "* `data` : 文本中的词语，这些词语以ID来代替 (e.g. [0, 1, 2, 3, 2, 4])\n",
    "\n",
    "标记 `UNK` 来表示稀有词语。\n",
    "\n",
    "词典中只统计50000个常见词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "837a7c33-2bc3-4a6c-b1e7-9ebaab18c2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 68751], ('the', 226893), (',', 184013), ('.', 120919), ('of', 116323)]\n",
      "Sample data [1721, 9, 8, 16479, 223, 4, 5168, 4459, 26, 11597]\n"
     ]
    }
   ],
   "source": [
    "# we restrict our vocabulary size to 50000\n",
    "vocabulary_size = 50000 \n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    # Gets only the vocabulary_size most common words as the vocabulary\n",
    "    # All the other words will be replaced with UNK token\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "\n",
    "    # Create an ID for each word by giving the current length of the dictionary\n",
    "    # And adding that item to the dictionary\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    # Traverse through all the text we have and produce a list\n",
    "    # where each element corresponds to the ID of the word found at that index\n",
    "    for word in words:\n",
    "        # If word is in the dictionary use the word ID,\n",
    "        # else use the ID of the special token \"UNK\"\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    \n",
    "    # update the count variable with the number of UNK occurences\n",
    "    count[0][1] = unk_count\n",
    "  \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    # Make sure the dictionary is of size of the vocabulary\n",
    "    assert len(dictionary) == vocabulary_size\n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae5c81e-35b9-4875-a0f1-ee06fff4a2f0",
   "metadata": {},
   "source": [
    "# 4 生成GloVe的batch数据\n",
    "\n",
    "`batch`是中心词；`labels`是中心词上下文窗口中的词语。对于中心词的上下文，每次读取`2 * window_size + 1`个词语，称之为`span`。每个`span`中，中心词为`1`，上下文大小为`2 * window_size`。该函数以这种方式继续，直到创建`batch_size`数据点。每次到达单词序列的末尾时，我们都会从头开始。\n",
    "\n",
    "`batch`: $1 \\times 8$的向量; `labels`: $8 \\times 1$的向量; `weights`: $1 \\times 8$的向量，词语$i$与词语$j$共现的次数，$\\frac{1}{d}$，其中$d$为两个词之间的距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9b3b685-2e0f-4fd9-af21-cf1bbc18e5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at']\n",
      "\n",
      "with window_size = 2:\n",
      "    batch: ['a', 'a', 'a', 'a', 'concerted', 'concerted', 'concerted', 'concerted']\n",
      "    labels: ['propaganda', 'is', 'concerted', 'set', 'is', 'a', 'set', 'of']\n",
      "    weights: [0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.5]\n",
      "\n",
      "with window_size = 4:\n",
      "    batch: ['set', 'set', 'set', 'set', 'set', 'set', 'set', 'set']\n",
      "    labels: ['propaganda', 'is', 'a', 'concerted', 'of', 'messages', 'aimed', 'at']\n",
      "    weights: [0.25, 0.33333334, 0.5, 1.0, 1.0, 0.5, 0.33333334, 0.25]\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, window_size):\n",
    "    # data_index is updated by 1 everytime we read a data point\n",
    "    global data_index \n",
    "    \n",
    "    # two numpy arras to hold target words (batch)\n",
    "    # and context words (labels)\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    weights = np.ndarray(shape=(batch_size), dtype=np.float32)\n",
    "\n",
    "    # span defines the total window size, where\n",
    "    # data we consider at an instance looks as follows. \n",
    "    # [ skip_window target skip_window ]\n",
    "    span = 2 * window_size + 1 \n",
    "    \n",
    "    # The buffer holds the data contained within the span\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "  \n",
    "    # Fill the buffer and update the data_index\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "  \n",
    "    # This is the number of context words we sample for a single target word\n",
    "    num_samples = 2*window_size \n",
    "\n",
    "    # We break the batch reading into two for loops\n",
    "    # The inner for loop fills in the batch and labels with \n",
    "    # num_samples data points using data contained withing the span\n",
    "    # The outper for loop repeat this for batch_size//num_samples times\n",
    "    # to produce a full batch\n",
    "    for i in range(batch_size // num_samples):\n",
    "        k=0\n",
    "        # avoid the target word itself as a prediction\n",
    "        # fill in batch and label numpy arrays\n",
    "        for j in list(range(window_size))+list(range(window_size+1,2*window_size+1)):\n",
    "            batch[i * num_samples + k] = buffer[window_size]\n",
    "            labels[i * num_samples + k, 0] = buffer[j]\n",
    "            # 因为 j 是跳过了 window_size 的，所以 j - window_size 不会为0\n",
    "            weights[i * num_samples + k] = abs(1.0/(j - window_size))\n",
    "            k += 1 \n",
    "    \n",
    "        # Everytime we read num_samples data points,\n",
    "        # we have created the maximum number of datapoints possible\n",
    "        # withing a single span, so we need to move the span by 1\n",
    "        # to create a fresh new span\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels, weights\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:9]])\n",
    "\n",
    "for window_size in [2, 4]:\n",
    "    data_index = 0\n",
    "    batch, labels, weights = generate_batch(batch_size=8, window_size=window_size)\n",
    "    print('\\nwith window_size = %d:' %window_size)\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])\n",
    "    print('    weights:', [w for w in weights])\n",
    "    \n",
    "    # print(batch)\n",
    "    # print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55fef8-d166-4c4e-ade7-fc78b49559f8",
   "metadata": {},
   "source": [
    "# 5 生成共现概率矩阵\n",
    "\n",
    "`lil_matrix(arg1, shape=None, dtype=None, copy=False)`, 基于行连接存储的稀疏矩阵。lil_matrix使用两个列表保存非零元素。data保存每行中的非零元素，rows保存非零元素所在的列。这种格式也很适合逐个添加元素，并且能快速获取行相关的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71baa9d9-c527-4ec4-bc95-babe2758513a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 50000)\n",
      "Running 420149 iterations to compute the co-occurance matrix\n",
      "\tFinished 100000 iterations\n",
      "\tFinished 200000 iterations\n",
      "\tFinished 300000 iterations\n",
      "\tFinished 400000 iterations\n",
      "Sample chunks of co-occurance matrix\n",
      "\n",
      "Target Word: \"UNK\"\n",
      "Context word:\",\"(id:2,count:3247.72), \"UNK\"(id:0,count:2071.51), \"the\"(id:1,count:1967.02), \"and\"(id:5,count:1425.50), \".\"(id:3,count:1304.42), \"of\"(id:4,count:1048.25), \"(\"(id:13,count:1008.75), \"in\"(id:6,count:802.75), \")\"(id:12,count:770.92), \"a\"(id:8,count:583.83), \n",
      "\n",
      "Target Word: \"engaging\"\n",
      "Context word:\"in\"(id:6,count:2.00), \",\"(id:2,count:1.25), \"from\"(id:21,count:1.00), \"every\"(id:326,count:1.00), \"remained\"(id:471,count:1.00), \"with\"(id:17,count:1.00), \"women\"(id:445,count:1.00), \"the\"(id:1,count:0.92), \"chickens\"(id:12727,count:0.50), \"traditionally\"(id:1854,count:0.50), \n",
      "\n",
      "Target Word: \"arguably\"\n",
      "Context word:\"the\"(id:1,count:1.50), \",\"(id:2,count:1.50), \"as\"(id:10,count:1.33), \".\"(id:3,count:1.33), \"is\"(id:9,count:1.00), \"can\"(id:57,count:1.00), \"entries\"(id:5951,count:1.00), \"be\"(id:30,count:1.00), \"most\"(id:46,count:0.83), \"nouns\"(id:4757,count:0.50), \n",
      "\n",
      "Target Word: \"amnesty\"\n",
      "Context word:\"the\"(id:1,count:2.00), \"international\"(id:163,count:2.00), \"of\"(id:4,count:1.50), \",\"(id:2,count:1.25), \"an\"(id:28,count:1.00), \".\"(id:3,count:1.00), \"program\"(id:638,count:1.00), \"for\"(id:15,count:1.00), \"general\"(id:178,count:1.00), \"has\"(id:35,count:0.67), \n",
      "\n",
      "Target Word: \"dolphin\"\n",
      "Context word:\"the\"(id:1,count:2.33), \"in\"(id:6,count:1.50), \"UNK\"(id:0,count:1.00), \"6\"(id:810,count:1.00), \",\"(id:2,count:1.00), \"patterns\"(id:2085,count:1.00), \"are\"(id:23,count:1.00), \"at\"(id:26,count:0.50), \"middle\"(id:476,count:0.33), \"confined\"(id:6776,count:0.33), \n",
      "\n",
      "Target Word: \"and\"\n",
      "Context word:\",\"(id:2,count:4013.62), \"the\"(id:1,count:2565.00), \"UNK\"(id:0,count:1420.09), \"of\"(id:4,count:997.83), \".\"(id:3,count:883.08), \"in\"(id:6,count:757.33), \"to\"(id:7,count:571.92), \"a\"(id:8,count:560.00), \")\"(id:12,count:400.08), \"and\"(id:5,count:321.42), \n",
      "\n",
      "Target Word: \"in\"\n",
      "Context word:\"the\"(id:1,count:3793.96), \",\"(id:2,count:1868.93), \".\"(id:3,count:1766.84), \"of\"(id:4,count:776.00), \"UNK\"(id:0,count:764.33), \"a\"(id:8,count:700.25), \"and\"(id:5,count:700.25), \"to\"(id:7,count:421.92), \"in\"(id:6,count:298.50), \"was\"(id:11,count:289.33), \n",
      "\n",
      "Target Word: \"to\"\n",
      "Context word:\"the\"(id:1,count:2481.16), \",\"(id:2,count:989.33), \".\"(id:3,count:689.00), \"a\"(id:8,count:579.83), \"and\"(id:5,count:573.08), \"be\"(id:30,count:553.83), \"of\"(id:4,count:470.50), \"UNK\"(id:0,count:470.00), \"in\"(id:6,count:412.25), \"is\"(id:9,count:283.42), \n",
      "\n",
      "Target Word: \"a\"\n",
      "Context word:\",\"(id:2,count:1437.76), \"of\"(id:4,count:1334.67), \".\"(id:3,count:899.33), \"in\"(id:6,count:689.08), \"the\"(id:1,count:651.25), \"as\"(id:10,count:613.67), \"to\"(id:7,count:586.08), \"UNK\"(id:0,count:584.17), \"and\"(id:5,count:542.08), \"is\"(id:9,count:536.75), \n",
      "\n",
      "Target Word: \"is\"\n",
      "Context word:\"the\"(id:1,count:1033.74), \",\"(id:2,count:625.75), \".\"(id:3,count:578.00), \"a\"(id:8,count:522.17), \"it\"(id:24,count:392.50), \"of\"(id:4,count:350.08), \"to\"(id:7,count:262.25), \"UNK\"(id:0,count:253.08), \"in\"(id:6,count:250.33), \"and\"(id:5,count:235.25), \n"
     ]
    }
   ],
   "source": [
    "# We are creating the co-occurance matrix as a compressed sparse colum matrix from scipy. \n",
    "cooc_data_index = 0\n",
    "dataset_size = len(data) # We iterate through the full text\n",
    "skip_window = 4 # How many words to consider left and right.\n",
    "\n",
    "# The sparse matrix that stores the word co-occurences\n",
    "cooc_mat = lil_matrix((vocabulary_size, vocabulary_size), dtype=np.float32)\n",
    "\n",
    "print(cooc_mat.shape)\n",
    "def generate_cooc(batch_size, skip_window):\n",
    "    '''\n",
    "    Generate co-occurence matrix by processing batches of data\n",
    "    '''\n",
    "    data_index = 0\n",
    "    print('Running %d iterations to compute the co-occurance matrix'%(dataset_size//batch_size))\n",
    "    for i in range(dataset_size//batch_size):\n",
    "        # Printing progress\n",
    "        if i>0 and i%100000==0:\n",
    "            print('\\tFinished %d iterations'%i)\n",
    "            \n",
    "        # Generating a single batch of data\n",
    "        batch, labels, weights = generate_batch(batch_size, skip_window)\n",
    "        labels = labels.reshape(-1)\n",
    "        \n",
    "        # Incrementing the sparse matrix entries accordingly\n",
    "        # inp: 中心词 i 的 id\n",
    "        # lbl: 上下文词语 j 的 id\n",
    "        # w: i 与 j 共现的频率\n",
    "        for inp,lbl,w in zip(batch,labels,weights):            \n",
    "            cooc_mat[inp,lbl] += (1.0*w)\n",
    "\n",
    "# Generate the matrix\n",
    "generate_cooc(8,skip_window)    \n",
    "\n",
    "# Just printing some parts of co-occurance matrix\n",
    "print('Sample chunks of co-occurance matrix')\n",
    "\n",
    "\n",
    "# Basically calculates the highest cooccurance of several chosen word\n",
    "for i in range(10):\n",
    "    idx_target = i\n",
    "    \n",
    "    # get the ith row of the sparse matrix and make it dense\n",
    "    ith_row = cooc_mat.getrow(idx_target)     \n",
    "    ith_row_dense = ith_row.toarray('C').reshape(-1)  # 获得频率，如果ith_row没有这个元素，那么就是0     \n",
    "    \n",
    "    # select target words only with a reasonable words around it.\n",
    "    # 获得一个 X_i 在 10 - 50000 之间的单词\n",
    "    while np.sum(ith_row_dense)<10 or np.sum(ith_row_dense)>50000:\n",
    "        # Choose a random word\n",
    "        idx_target = np.random.randint(0,vocabulary_size)\n",
    "        \n",
    "        # get the ith row of the sparse matrix and make it dense\n",
    "        ith_row = cooc_mat.getrow(idx_target) \n",
    "        ith_row_dense = ith_row.toarray('C').reshape(-1)    \n",
    "        \n",
    "    print('\\nTarget Word: \"%s\"'%reverse_dictionary[idx_target])\n",
    "        \n",
    "    # sort_indices 按照从小到大排序 ith_row_dense (词频从小到大排序), 结果为索引\n",
    "    sort_indices = np.argsort(ith_row_dense).reshape(-1) # indices with highest count of ith_row_dense\n",
    "    # 按照从大到小排序 ith_row_dense (词频从大到小排序), 结果为索引\n",
    "    sort_indices = np.flip(sort_indices,axis=0) # reverse the array (to get max values to the start)\n",
    "\n",
    "    # printing several context words to make sure cooc_mat is correct\n",
    "    print('Context word:',end='')\n",
    "    for j in range(10):        \n",
    "        idx_context = sort_indices[j]       \n",
    "        print('\"%s\"(id:%d,count:%.2f), '%(reverse_dictionary[idx_context],idx_context,ith_row_dense[idx_context]),end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551fbd1a-f585-49dd-99f1-d2674d745328",
   "metadata": {},
   "source": [
    "# 6 GloVe 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a0a23-b8ca-42d6-ae39-2e24873c1883",
   "metadata": {},
   "source": [
    "## 6.1 定义超参数\n",
    "\n",
    "`batch_size`: 每个 batch 中的样本数；`embedding_size`: 嵌入层向量的大小；`window_size`: 上下文窗口大小；`valid_examples`: 随机选择的验证集样本（随机选择后就是常量了）； `epsilon`: 防止 ${\\rm log}$ 发散。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a382fec-8535-49d2-9335-5e5e1a00929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # Data points in a single batch\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "window_size = 4 # How many words to consider left and right.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid datapoints randomly from a large window without always being deterministic\n",
    "valid_window = 50\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words as well\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_examples = np.append(valid_examples,random.sample(range(1000, 1000+valid_window), valid_size),axis=0)\n",
    "\n",
    "num_sampled = 32 # Number of negative examples to sample.\n",
    "\n",
    "epsilon = 1 # used for the stability of log in the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6d693c-9a93-426c-b31c-fd23a7855ce2",
   "metadata": {},
   "source": [
    "## 6.2 定义输入与输出\n",
    "\n",
    "为每一个`batch_size`的内容创建训练集中输入与输出的`placeholders`，并且为验证集创建一个常数的tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0595fbd2-5b51-4458-8f67-f9c2c0b72773",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training input data (target word IDs).\n",
    "train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "# Training input label data (context word IDs)\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "# Validation input data, we don't need a placeholder\n",
    "# as we have already defined the IDs of the words selected\n",
    "# as validation data\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900f6b83-b49e-4877-a87e-a12de5005246",
   "metadata": {},
   "source": [
    "## 6.3 定义模型参数以及其他变量\n",
    "\n",
    "`in_embeddings`: $W$, $50000 \\times 128$; `in_bias_embeddings`: $b$, $50000 \\times 1$; `out_embeddings`: $\\tilde{W}$, $50000 \\times 128$; `out_bias_embeddings`: $\\tilde{b}$, $50000$\n",
    "\n",
    "词向量初始化都是$[-1, 1]$的均匀分布，偏置初始化都是$[0, 0.01]$的均匀分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71ff32e8-39be-4c47-9d3a-f1f285183284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables.\n",
    "in_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name='embeddings')\n",
    "in_bias_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size],0.0,0.01,dtype=tf.float32),name='embeddings_bias')\n",
    "\n",
    "out_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name='embeddings')\n",
    "out_bias_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size],0.0,0.01,dtype=tf.float32),name='embeddings_bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b3307-2a15-4b02-bc18-91ac3c26ff53",
   "metadata": {},
   "source": [
    "## 6.4 定义模型计算\n",
    "\n",
    "定义了4个查找方法：`embed_in`, `embed_out`, `embed_bias_in`, `embed_bias_out`。\n",
    "\n",
    "`weights_x`: $1 \\times 8$, 权重函数 $f(X_{ij})$\n",
    "\n",
    "`x_ij`: $1 \\times 8$, 词语 $i$ 与 $j$ 的共现频率, $X_{ij}$\n",
    "\n",
    "损失函数：$J=\\sum_{i, j=1}^V f(X_{ij}) (w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j - {\\rm log}(1 + X_{ij}))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ffc7f6c-79ef-45de-91cc-4d1fb91b75bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up embeddings for inputs and outputs\n",
    "# Have two seperate embedding vector spaces for inputs and outputs\n",
    "embed_in = tf.nn.embedding_lookup(in_embeddings, train_dataset)\n",
    "embed_out = tf.nn.embedding_lookup(out_embeddings, train_labels)\n",
    "embed_bias_in = tf.nn.embedding_lookup(in_bias_embeddings,train_dataset)\n",
    "embed_bias_out = tf.nn.embedding_lookup(out_bias_embeddings,train_labels)\n",
    "\n",
    "# weights used in the cost function\n",
    "weights_x = tf.placeholder(tf.float32,shape=[batch_size],name='weights_x') \n",
    "# Cooccurence value for that position\n",
    "x_ij = tf.placeholder(tf.float32,shape=[batch_size],name='x_ij')\n",
    "\n",
    "# Compute the loss defined in the paper. Note that \n",
    "# I'm not following the exact equation given (which is computing a pair of words at a time)\n",
    "# I'm calculating the loss for a batch at one time, but the calculations are identical.\n",
    "# I also made an assumption about the bias, that it is a smaller type of embedding\n",
    "loss = tf.reduce_mean(\n",
    "    weights_x * (tf.reduce_sum(embed_in*embed_out,axis=1) + embed_bias_in + embed_bias_out - tf.log(epsilon+x_ij))**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884bf33-ef02-49cd-968b-c07c2caa609d",
   "metadata": {},
   "source": [
    "## 6.5 相似度计算\n",
    "\n",
    "采用余弦相似度计算词语的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0443c6e5-7b09-42a2-bf5f-714ec5edd2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the similarity between minibatch examples and all embeddings.\n",
    "# We use the cosine distance:\n",
    "embeddings = (in_embeddings + out_embeddings)/2.0  # X = U + V\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))  # 矩阵中每行元素的模\n",
    "normalized_embeddings = embeddings / norm  # L2正则化\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)  # 提取验证集中的数据\n",
    "similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))  # 余弦相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a924fd-fe2b-4bee-aae5-ce6e0b6b6f4e",
   "metadata": {},
   "source": [
    "## 6.6 定义模型参数优化器\n",
    "\n",
    "采用Adagrad优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "840fb96d-a47b-41e9-900b-9e4b3807ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer.\n",
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f4764-71aa-48c8-9c57-fcdb1af3060f",
   "metadata": {},
   "source": [
    "## 6.7 运行GloVe模型\n",
    "\n",
    "训练数据，训练`num_steps`次。并且在每次迭代中，在一个固定的验证集中评估算法，并且打印出距离给定词语最近的词语。\n",
    "\n",
    "从结果来看，随着训练的进行，最接近验证集中词语的词语是一直在发生改变的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8797df0-aa07-4533-ab54-b8ab115761bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 8.672687\n",
      "Nearest to ,: pitcher, discharges, pigs, tolerant, fuzzy, medium-, on-campus, eduskunta,\n",
      "Nearest to this: mediastinal, destined, implementing, honolulu, non-mormon, juniors, tycho, powered,\n",
      "Nearest to most: translating, absolute, 111, bechet, adam, aleksey, penetrators, rake,\n",
      "Nearest to but: motown, ridged, beginnings, shareholder, resurfacing, english, intelligence, o'dea,\n",
      "Nearest to is: higher-quality, kitchener, kelley, confronted, m15, stanislaus, depictions, buf,\n",
      "Nearest to ): encyclopedic, commute, symbiotic, forecasts, 1993., 243-year, cenwealh, inclosure,\n",
      "Nearest to not: toulon, discount, dunblane, vividly, recorded, olive, afrikaansche, german-speaking,\n",
      "Nearest to with: tofu, expansive, penned, grids, 102, drought, merced, cunningham,\n",
      "Nearest to ;: all-electric, internationally-recognised, czars, 12–16, kana, immaculate, innings, wnba,\n",
      "Nearest to a: non-residents, presumption, cephas, tau, stepfather, beside, aorist, vom,\n",
      "Nearest to for: bitterroots, sx-64, weekday, edificio, sousley, self-proclaimed, whoever, liquid,\n",
      "Nearest to have: dissenting, barret, psilocybin, massamba-débat, kopfstein, 5.5, fillmore, innovator,\n",
      "Nearest to was: ., is, most, wheelchair, 1575, warm-blooded, dynamically, 1913.,\n",
      "Nearest to 's: eoka, melancholia, downs, gallipoli, reichswehr, easter, chest, construed,\n",
      "Nearest to were: 1138, djuna, 3, beni, high-grade, slander, agency, séamus,\n",
      "Nearest to be: knelt, horrors, assistant, hospitalised, 1802, fierce, cinemas, magnified,\n",
      "Average loss at step 2000: 0.871986\n",
      "Average loss at step 4000: 0.104019\n",
      "Average loss at step 6000: 0.078603\n",
      "Average loss at step 8000: 0.068353\n",
      "Average loss at step 10000: 0.062539\n",
      "Nearest to ,: the, ., in, of, a, ,, and, is,\n",
      "Nearest to this: ), UNK, (, ``, ., ,, and, in,\n",
      "Nearest to most: ., the, of, and, ,, a, to, for,\n",
      "Nearest to but: ), UNK, ,, and, a, ., in, the,\n",
      "Nearest to is: in, of, were, higher-quality, by, the, one—a, kelley,\n",
      "Nearest to ): is, ., in, ,, and, were, the, was,\n",
      "Nearest to not: loudly, german-speaking, interagency, afrikaansche, jumpers, UNK, (, offences,\n",
      "Nearest to with: been, to, has, a, may, had, were, not,\n",
      "Nearest to ;: a, ,, well, such, and, UNK, for, of,\n",
      "Nearest to a: the, ,, ., and, in, was, a, of,\n",
      "Nearest to for: and, are, ,, by, the, ., in, was,\n",
      "Nearest to have: is, was, that, there, has, ., a, are,\n",
      "Nearest to was: ., of, in, and, ,, a, by, to,\n",
      "Nearest to 's: is, are, it, there, was, ., has, that,\n",
      "Nearest to were: a, to, and, as, ,, in, UNK, was,\n",
      "Nearest to be: was, that, is, it, ,, in, ., for,\n",
      "Average loss at step 12000: 0.618814\n",
      "Average loss at step 14000: 0.062696\n",
      "Average loss at step 16000: 0.055186\n",
      "Average loss at step 18000: 0.050817\n",
      "Average loss at step 20000: 0.056369\n",
      "Nearest to ,: in, ., the, of, a, ,, and, is,\n",
      "Nearest to this: ), (, ``, ., in, ,, and, the,\n",
      "Nearest to most: ., the, of, ,, and, for, a, 's,\n",
      "Nearest to but: ), and, ,, UNK, '', in, the, .,\n",
      "Nearest to is: 's, of, the, were, ., in, by, for,\n",
      "Nearest to ): is, were, ., in, ,, the, was, and,\n",
      "Nearest to not: (, loudly, ), 's, jumpers, ``, the, .,\n",
      "Nearest to with: been, to, be, had, has, a, were, not,\n",
      "Nearest to ;: a, ,, such, for, well, and, is, with,\n",
      "Nearest to a: the, ., was, in, ,, and, a, of,\n",
      "Nearest to for: are, and, by, ,, in, was, ., the,\n",
      "Nearest to have: is, was, that, a, has, not, this, .,\n",
      "Nearest to was: ., of, in, and, 's, ,, a, by,\n",
      "Nearest to 's: is, it, are, there, was, has, a, .,\n",
      "Nearest to were: a, as, to, and, ,, is, in, with,\n",
      "Nearest to be: was, it, that, his, ,, had, in, is,\n",
      "Average loss at step 22000: 0.049327\n",
      "Average loss at step 24000: 0.045660\n",
      "Average loss at step 26000: 0.043688\n",
      "Average loss at step 28000: 0.041446\n",
      "Average loss at step 30000: 0.040662\n",
      "Nearest to ,: the, in, ., of, a, ,, and, to,\n",
      "Nearest to this: ), (, ``, ., in, ,, and, UNK,\n",
      "Nearest to most: ., the, of, ,, and, for, a, by,\n",
      "Nearest to but: ), UNK, '', and, ,, or, in, .,\n",
      "Nearest to is: of, 's, the, ., in, on, were, world,\n",
      "Nearest to ): were, is, in, ., ,, and, the, by,\n",
      "Nearest to not: (, ``, ), of, ., the, loudly, UNK,\n",
      "Nearest to with: been, to, had, has, be, a, that, were,\n",
      "Nearest to ;: a, ,, such, and, with, for, is, an,\n",
      "Nearest to a: the, ., in, was, ,, and, a, of,\n",
      "Nearest to for: are, and, ,, by, in, was, the, .,\n",
      "Nearest to have: is, was, that, has, also, not, this, a,\n",
      "Nearest to was: of, ., in, and, ,, 's, for, a,\n",
      "Nearest to 's: it, is, are, has, there, was, this, a,\n",
      "Nearest to were: a, as, and, is, to, ,, with, was,\n",
      "Nearest to be: was, it, his, that, had, in, ,, is,\n",
      "Average loss at step 32000: 0.039611\n",
      "Average loss at step 34000: 0.037096\n",
      "Average loss at step 36000: 0.031070\n",
      "Average loss at step 38000: 0.031204\n",
      "Average loss at step 40000: 0.029573\n",
      "Nearest to ,: ., the, in, of, a, and, ,, with,\n",
      "Nearest to this: ), (, ``, ., in, ,, and, UNK,\n",
      "Nearest to most: ., the, of, ,, and, for, a, to,\n",
      "Nearest to but: ), UNK, '', ,, and, or, in, .,\n",
      "Nearest to is: 's, the, of, in, ., at, world, on,\n",
      "Nearest to ): were, in, ., is, ,, the, and, by,\n",
      "Nearest to not: (, ``, ), UNK, 's, the, ., of,\n",
      "Nearest to with: been, had, to, has, be, a, that, were,\n",
      "Nearest to ;: a, such, ,, for, and, an, with, is,\n",
      "Nearest to a: the, ., in, was, ,, and, a, of,\n",
      "Nearest to for: are, by, and, in, ,, the, was, .,\n",
      "Nearest to have: is, was, that, also, this, not, has, a,\n",
      "Nearest to was: ., of, in, and, ,, 's, by, a,\n",
      "Nearest to 's: it, is, has, are, there, this, was, a,\n",
      "Nearest to were: a, as, and, with, ,, is, to, for,\n",
      "Nearest to be: was, it, his, that, had, ,, in, which,\n",
      "Average loss at step 42000: 0.030051\n",
      "Average loss at step 44000: 0.028354\n",
      "Average loss at step 46000: 0.027985\n",
      "Average loss at step 48000: 0.028454\n",
      "Average loss at step 50000: 0.026475\n",
      "Nearest to ,: the, ., in, of, a, and, ,, to,\n",
      "Nearest to this: ), ``, (, UNK, ., in, ,, 's,\n",
      "Nearest to most: ., the, of, ,, and, for, to, a,\n",
      "Nearest to but: ), UNK, '', and, ,, or, in, .,\n",
      "Nearest to is: of, the, 's, ., in, at, world, on,\n",
      "Nearest to ): were, in, ., ,, and, the, is, by,\n",
      "Nearest to not: (, ``, UNK, ), of, the, and, .,\n",
      "Nearest to with: been, had, to, has, be, a, that, were,\n",
      "Nearest to ;: a, ,, such, for, an, and, with, ``,\n",
      "Nearest to a: the, in, was, ., ,, and, a, of,\n",
      "Nearest to for: are, and, by, in, ,, the, to, .,\n",
      "Nearest to have: is, was, that, this, also, not, has, he,\n",
      "Nearest to was: ., of, in, and, ,, 's, a, for,\n",
      "Nearest to 's: is, it, has, this, there, are, was, not,\n",
      "Nearest to were: a, as, is, and, ,, for, to, with,\n",
      "Nearest to be: was, it, had, that, his, is, ,, in,\n",
      "Average loss at step 52000: 0.025816\n",
      "Average loss at step 54000: 0.028593\n",
      "Average loss at step 56000: 0.025053\n",
      "Average loss at step 58000: 0.025562\n",
      "Average loss at step 60000: 0.025235\n",
      "Nearest to ,: ., the, in, of, a, and, ,, to,\n",
      "Nearest to this: ), (, ``, UNK, ., in, or, ,,\n",
      "Nearest to most: ., the, of, ,, and, for, to, by,\n",
      "Nearest to but: ), UNK, '', ,, and, or, in, .,\n",
      "Nearest to is: 's, the, of, ., in, world, at, on,\n",
      "Nearest to ): were, ., in, ,, and, the, is, by,\n",
      "Nearest to not: (, ``, UNK, ), '', of, the, 's,\n",
      "Nearest to with: been, had, to, be, has, a, that, were,\n",
      "Nearest to ;: a, such, ,, an, for, and, is, ``,\n",
      "Nearest to a: the, ., was, in, ,, and, a, of,\n",
      "Nearest to for: are, by, and, ,, in, the, was, to,\n",
      "Nearest to have: is, was, that, this, also, has, not, he,\n",
      "Nearest to was: ., of, in, and, ,, 's, for, by,\n",
      "Nearest to 's: is, it, has, there, are, this, not, was,\n",
      "Nearest to were: a, as, is, and, ,, with, to, for,\n",
      "Nearest to be: was, it, had, that, his, in, when, ,,\n",
      "Average loss at step 62000: 0.025567\n",
      "Average loss at step 64000: 0.024250\n",
      "Average loss at step 66000: 0.024251\n",
      "Average loss at step 68000: 0.023763\n",
      "Average loss at step 70000: 0.023084\n",
      "Nearest to ,: ., the, in, of, a, and, ,, to,\n",
      "Nearest to this: ), (, ``, UNK, ., or, in, ,,\n",
      "Nearest to most: ., the, of, ,, and, for, by, a,\n",
      "Nearest to but: ), UNK, '', or, ,, and, in, .,\n",
      "Nearest to is: the, 's, of, in, ., world, at, on,\n",
      "Nearest to ): were, ., in, ,, and, is, the, by,\n",
      "Nearest to not: (, ``, UNK, ), '', the, 's, of,\n",
      "Nearest to with: been, had, to, has, be, a, that, was,\n",
      "Nearest to ;: a, such, ,, an, for, and, is, with,\n",
      "Nearest to a: the, was, ., in, ,, and, of, a,\n",
      "Nearest to for: are, by, and, ,, in, the, was, to,\n",
      "Nearest to have: is, was, that, this, also, not, has, a,\n",
      "Nearest to was: ., of, in, and, ,, 's, for, a,\n",
      "Nearest to 's: it, is, has, are, there, this, was, not,\n",
      "Nearest to were: as, a, and, is, ,, for, with, to,\n",
      "Nearest to be: was, it, had, his, that, when, in, ,,\n",
      "Average loss at step 72000: 0.023079\n",
      "Average loss at step 74000: 0.022509\n",
      "Average loss at step 76000: 0.035127\n",
      "Average loss at step 78000: 0.021737\n",
      "Average loss at step 80000: 0.021943\n",
      "Nearest to ,: ., the, in, a, of, and, ,, to,\n",
      "Nearest to this: ), (, ``, UNK, or, ., in, ,,\n",
      "Nearest to most: ., the, of, ,, and, for, to, by,\n",
      "Nearest to but: ), UNK, '', or, and, ,, in, .,\n",
      "Nearest to is: 's, the, of, ., in, world, at, on,\n",
      "Nearest to ): were, in, ., and, ,, is, the, by,\n",
      "Nearest to not: (, ``, UNK, ), 's, '', the, of,\n",
      "Nearest to with: been, had, to, has, be, a, that, may,\n",
      "Nearest to ;: a, such, an, ,, for, and, is, with,\n",
      "Nearest to a: the, ., was, in, ,, and, of, a,\n",
      "Nearest to for: are, by, and, ,, in, the, was, .,\n",
      "Nearest to have: is, was, that, also, this, not, has, he,\n",
      "Nearest to was: ., of, in, and, ,, for, 's, by,\n",
      "Nearest to 's: it, is, has, there, are, this, was, not,\n",
      "Nearest to were: a, as, is, with, and, for, ,, to,\n",
      "Nearest to be: was, it, had, when, his, that, in, ,,\n",
      "Average loss at step 82000: 0.023584\n",
      "Average loss at step 84000: 0.021502\n",
      "Average loss at step 86000: 0.020836\n",
      "Average loss at step 88000: 0.021815\n",
      "Average loss at step 90000: 0.020914\n",
      "Nearest to ,: ., the, in, a, of, and, ,, to,\n",
      "Nearest to this: ), (, ``, UNK, or, ., in, ,,\n",
      "Nearest to most: ., the, ,, of, and, for, on, to,\n",
      "Nearest to but: ), UNK, '', or, and, ,, in, .,\n",
      "Nearest to is: 's, the, of, ., at, in, world, on,\n",
      "Nearest to ): were, ., in, and, ,, the, is, to,\n",
      "Nearest to not: (, ``, UNK, ), '', 's, the, of,\n",
      "Nearest to with: been, had, to, has, be, that, a, may,\n",
      "Nearest to ;: a, such, an, ,, for, and, is, ``,\n",
      "Nearest to a: the, was, ., in, and, ,, of, to,\n",
      "Nearest to for: are, and, by, ,, in, to, the, .,\n",
      "Nearest to have: is, was, that, also, this, not, has, a,\n",
      "Nearest to was: ., of, in, and, ,, for, 's, a,\n",
      "Nearest to 's: it, is, has, there, this, are, was, not,\n",
      "Nearest to were: as, a, is, for, and, to, ,, with,\n",
      "Nearest to be: was, it, had, when, that, his, in, ,,\n",
      "Average loss at step 92000: 0.020411\n",
      "Average loss at step 94000: 0.020283\n",
      "Average loss at step 96000: 0.019547\n",
      "Average loss at step 98000: 0.019270\n",
      "Average loss at step 100000: 0.019544\n",
      "Nearest to ,: ., the, in, a, of, and, ,, is,\n",
      "Nearest to this: ), (, ``, UNK, or, ., in, ,,\n",
      "Nearest to most: ., the, of, ,, and, for, a, to,\n",
      "Nearest to but: ), UNK, '', or, and, ,, in, .,\n",
      "Nearest to is: 's, the, of, at, world, ., in, on,\n",
      "Nearest to ): were, in, ., and, ,, the, by, is,\n",
      "Nearest to not: (, ``, UNK, ), '', of, 's, the,\n",
      "Nearest to with: been, had, to, has, be, that, a, may,\n",
      "Nearest to ;: a, such, an, ,, for, and, with, is,\n",
      "Nearest to a: the, was, ., in, and, ,, to, of,\n",
      "Nearest to for: are, by, and, ,, in, to, the, was,\n",
      "Nearest to have: is, was, that, also, this, not, has, a,\n",
      "Nearest to was: ., of, in, and, ,, 's, for, to,\n",
      "Nearest to 's: it, is, has, there, this, are, was, not,\n",
      "Nearest to were: a, as, is, with, and, ,, to, for,\n",
      "Nearest to be: was, it, when, had, that, his, in, ,,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "glove_loss = []\n",
    "\n",
    "average_loss = 0\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        # generate a single batch (data,labels,co-occurance weights)\n",
    "        batch_data, batch_labels, batch_weights = generate_batch(\n",
    "            batch_size, skip_window) \n",
    "        \n",
    "        # 因为已经计算出来了共现矩阵，所以这里不需要 batch_weights\n",
    "        # Computing the weights required by the loss function\n",
    "        batch_weights = [] # weighting used in the loss function\n",
    "        batch_xij = [] # weighted frequency of finding i near j\n",
    "        \n",
    "        # Compute the weights for each datapoint in the batch\n",
    "        for inp,lbl in zip(batch_data,batch_labels.reshape(-1)):  \n",
    "            # 100: x_max, 0.75: 3/4, point_weight: f(X_ij), batch_xij: 词语 i 与 j 的频率\n",
    "            point_weight = (cooc_mat[inp,lbl]/100.0)**0.75 if cooc_mat[inp,lbl]<100.0 else 1.0 \n",
    "            batch_weights.append(point_weight)\n",
    "            batch_xij.append(cooc_mat[inp,lbl])\n",
    "        batch_weights = np.clip(batch_weights,-100,1)\n",
    "        batch_xij = np.asarray(batch_xij)\n",
    "        \n",
    "        # Populate the feed_dict and run the optimizer (minimize loss)\n",
    "        # and compute the loss. Specifically we provide\n",
    "        # train_dataset/train_labels: training inputs and training labels\n",
    "        # weights_x: measures the importance of a data point with respect to how much those two words co-occur\n",
    "        # x_ij: co-occurence matrix value for the row and column denoted by the words in a datapoint\n",
    "        feed_dict = {train_dataset : batch_data.reshape(-1), train_labels : batch_labels.reshape(-1),\n",
    "                     weights_x:batch_weights,x_ij:batch_xij}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        # Update the average loss variable\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            glove_loss.append(average_loss)\n",
    "            average_loss = 0\n",
    "        \n",
    "        # Here we compute the top_k closest words for a given validation word\n",
    "        # in terms of the cosine distance\n",
    "        # We do this for all the words in the validation set\n",
    "        # Note: This is an expensive step\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "            \n",
    "    final_embeddings = normalized_embeddings.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf18",
   "language": "python",
   "name": "tf18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
